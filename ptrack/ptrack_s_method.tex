\section{Methodology}
The biggest challenge for tracking large time-varying volumetric data set lies in that, though features can be extracted within individual processors using the conventional methods, they might also span over multiple data blocks, which is unavoidable as the number of processors increases. To extract and trace a feature over a distributed volume data set, we need to build and maintain the connectivity information of the features across multiple nodes. As feature descriptors, such as size, curvature, and velocity, are distributed among processors, connectivity information can facilitate us to obtain the description of a feature from neighboring processors, and also enable more advanced operations such as similarity evaluation.

However, it typically requires data exchanges among processors to build and maintain such connectivity information, and thus incurs extra communication cost. To design a proper communication scheme for better performance and scalability, we carefully consider the following three factors in our design:

\begin{itemize}
	\item $N_{com}$ : The amount of communication required to build the connectivity graph;
	\item $N_{proc/com}$ : The number of processors involved in each communication;
	\item $N_{data/com}$ : The amount of data that must be changed in each communication.
\end{itemize}

In the following sections, we give a detailed description on how to create and maintain such connectivity information using tree structures and then merge them into an undirected unweighted graph to minimize the cost over the above three factors.

\subsection{Feature Extraction}

In general, a feature can be any interesting object, structure or pattern that is considered relevant for investigation. Here, a feature is defined as the collection of voxices encompassed by a certain iso-surface. Such volumetric feature could be extracted by conventional techniques such as region growing, geometry or topology based clustering, or other domain specific algorithms. In our work we use a standard region-growing algorithm
%\textcolor{red}{why?} %
\cite{Huang2003} to partition the original volume data into an initial set of features. This can be done by first spreading a set of seeding points inside the volume, and then clustering voxices into separate regions, each regarded as a single feature. After specific features have been identified from a single time step, we can track their evolution over time using a prediction-correction approach \cite{Muelder2009}, as feature locations should be consistent for consecutive time step provided that the sampling interval is sufficiently small. Once the prediction is made, the actual region can be obtained by adjusting the surface of the predicted region: first shrink the edge surface points to obtain the mutual region between consecutive time steps, and then use of a region-growing method to generate the refined region, as depicted in Figure~\ref{fig:predict-correct}. This prediction-correction approach was proved effective and efficient for tracking feature on a single processor \cite{Muelder2009}. However, when the size of the volume becomes too large to be able to fit into a single processor, a cluster of processors is often needed so as to process the data in parallel.

%------------------------------------------------
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{predict_correct.png}
	\caption{The predict-correct feature tracking approach. First the predicted region is derived using location(s) of that feature in previous time step $t_{i-1}$. Then we shrink the portion that does not satisfy the criteria in $t_i$, which obtain us the mutual portion of that in $t_{i-1}$ and $t_i$. Finally we apply region growing again to obtain the expected feature region depicted.}
	\label{fig:predict-correct}
\end{figure}
%------------------------------------------------

One challenge to parallelize the tracking approach lies in that it can be difficult to obtain the global feature descriptions unless they can be shared and merged in an efficient way. This is because the features may span over multiple processors and partial features in different data blocks are operated independently. An intuitive way of exchanging such feature information is to first find how features span over multiple processors, and then merge them according to the connectivity information.

\subsection{Creating Local Connectivity Tree}

If we partition a volumetric data set into a regular grid of data blocks, it is very likely that some of the features will cross multiple blocks. Leveraging that the cross-section of such feature in both side of adjacent blocks should match, we could connect separate parts of a feature in adjacent blocks by comparing their cross-sections on correspondent boundary surfaces.

Since data is distributed, each processor is not aware of the partial features identified on the other processors. Though exchanging the voxels on the sectional area between two adjacent processors would be sufficient for finding possible matches of partial features, we choose to exchange more abstract data to find matches:

\begin{itemize}
	\item $P_{centroid}$: The geometric centroid of the cross-sectional area.
	\item $P_{min-max}$: The minimal and maximal (min-max) coordinate of the cross-sectional area;
\end{itemize}

The reason why we choose geometric centroid instead of introducing a voxel-width "ghost surface" is that it requires much less communication cost. A ghost surface that stores boundary surface belonging to neighboring blocks might help to achieve voxel-wise matching for partial features. However, maintaining such ghost surfaces requires frequent inter-process communication and is considerably expensive for data generated in real-time. Consider the fact that it is rarely the case that a feature will have a sharp \textcolor{red}{half-dome-like} sectional plane right on the boundary surface, we can loose the matching criterion by allowing a 1-voxel offset between two correspondent geometric centroids. Together with the min-max coordinate of the cross-sectional area, bipartite matching of partial features could be achieved, as shown in Algorithm~\ref{alg:match}.

%------------------------------------------------
\begin{algorithm}
\caption{Match of two partial features}
	\begin{algorithmic}
		\IF{$P_{centroid}$ = $P_{centroid}^{'}$ \textbf{and} $P_{min-max}$ = $P_{min-max}^{'}$}
			\STATE return $f$ \textbf{matches} $f^{'}$
		\ENDIF
	\end{algorithmic}
\label{alg:match}
\end{algorithm}
%------------------------------------------------

Another reason the min-max coordinate values are not optional is because they ensure correct connectivity for some special cases where one cross-sectional area is surrounded by another concave or hollow area whose centroid points happen to be the same, as depicted in Figure~\ref{fig:special}.

%------------------------------------------------
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{figure1@2x.png}
	\caption{A special case where two features share the same centroid on the section.}
	\label{fig:special}
\end{figure}
%------------------------------------------------

Based on the afore-explained fact that partial features could be connected by finding matches, we can abstract the local connectivity information using a tree structure as depicted in Figure~\ref{fig:match}. For each data block in the grid, there will be six direct neighbors (the outermost blocks have less), each with a sharing boundary surface with the current block. The connectivity tree is constructed taking the current block as root, its six adjacent blocks as its first level child nodes, and a new leaf is appended to each surface node if a local feature touches the block boundary surface.

%------------------------------------------------
\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{match.png}
	\caption{The tree structure used for maintaining local connectivity information, where the root node is encoded with the current block index, its child nodes the index of its neighboring blocks, and for each child node its leaves represent local on-boundary-features. The leaves should match that reside in the corespondent neighboring block, leveraging which a connectivity graph can be constructed.}
	\label{fig:match}
\end{figure}
%------------------------------------------------

Note that each voxel in the local data block has a unique global index, each leaf could thus be encoded using 3 integers (global index for $P_{centroid}$, $P_{min}$ and $P_{max}$), we use the index of $P_{centroid}$ as the feature index, and sort the sibling leaves according to its index in ascending order. On the other hand, the root and the first level child nodes can be encoded with the index of corresponding data blocks, which is irrelevant to the number of feature-on-boundary (henceforth referred as $N_{fb}$). Therefore the overall spatial complexity of creating local connectivity tree for each data block is ($\theta(3*N_{fb})$). \textcolor{red}{Consider an extreme case that there are 10k features touching each side of the boundary surface, the total memory needed for the local connectivity tree is only $((3*10000 + 1)*6 + 1) * 4 bytes \approx 720KB$, neglectable comparing to the volume size itself.}

From the temporal complexity perspective of view, the creation of local connectivity tree will not introduce extra computational cost as it can be done along with the region growing process. The value of $P_{centroid}$ and $P_{min-max}$ are updated only if the feature reaches the boundary surface, hence the temporal complexity for creating the local connectivity tree remains ${O(\sqrt[2/3]{N_{vexol}})}$, a magnitude less than \textcolor{red}{$O(extraction)$.}

Algorithm~\ref{alg:local} shows the detailed algorithm of creating local connectivity tree in the region growing process.
%------------------------------------------------
\begin{algorithm}
\caption{Creating Local Connectivity Tree}
\label{alg:local}

\begin{algorithmic}
	\IF{$t = t_0$}
		\STATE $seeds \leftarrow randomVortices()$
		\FOR {each $seed$ in $seeds$}
			\STATE $feature \leftarrow expendRegion()$
			\STATE append $feature$ to $featureList$
		\ENDFOR	
	\ELSE
		\FOR {each $feature$ in $featureList$}
			\STATE $feature \leftarrow predictRegion()$
			\STATE $feature,P_{centroid},P_{min-max} \leftarrow \textbf{adjustRegion()}$
			
			\STATE $leaf \leftarrow LEAF(P_{centroid}, P_{min-max})$
			\STATE append $leaf$ to $connectivityTree$
		\ENDFOR
	\ENDIF
\end{algorithmic}

\begin{algorithmic} \STATE \end{algorithmic}	% line separator

\begin{algorithmic}
\STATE $\textbf{adjustRegion:}$
	\IF{Voxel $v$ on boundary surface}
		\STATE $P_{centroid} \leftarrow updateBoundaryCentroid()$
		\STATE $P_{min-max} \leftarrow updateMinMaxBoundary()$
	\ENDIF
\end{algorithmic}
\end{algorithm}
%------------------------------------------------

\subsection{Creating Global Connectivity Graph}

After local connectivity trees have been created within each data block, their leaves need to be exchanged and merged to obtain the overall description of a partitioned feature. The exchanging and merging process is decisive that their effectiveness will largely affect the overall performance and scalability of the feature tracking algorithm as a whole.

In this subsection, we start with a naive solution and discuss progressively through different data exchanging strategy for different scenarios.

\subsubsection{The Naive Solution}

A naive solution to obtaining global connectivity information for each feature-on-boundary is to exchange its corresponding leaf with its targeting block. Recall that the first level child nodes are encoded as the ranks of adjacent blocks, and the leaves the global index of the geometric centroid and the min-max boundary on the shared boundary surface. Therefore if a leaf received from neighboring data block matches a local leaf, the two leaves, that is, the two partial features they represented, must share the same boundary section with the same centroid and section region. In other word, these two partial features are resulted by partitioning a original feature, and should be considered as the same feature.

To prevent a leaf from being resending to the same neighboring block multiple times, it is marked as sent and unless the same feature was found partially reside in another neighboring block, this leaf will be ignored in the next communication to reduce $N_{data/com}$.

Algorithm~\ref{alg:merge} shows the detailed process to merge matched leaves.
%------------------------------------------------
\begin{algorithm}
\caption{Merging Matched Leaves}
\label{alg:merge}
\begin{algorithmic}
\STATE traverse local connectivity tree in preorder
\FOR {each $f_{local}$ in $localLeaves$}
	\IF{$f_{local}$ is sent = false}
		\STATE send $f_{local}$ to targeting block
	\ENDIF
	\STATE mark $f_{local}$ as sent
\ENDFOR
\FOR {each $f_{recv}$ in $recievedLeaves$}
	\FOR {each $f_{local}$ in $localLeaves$}
		\IF{$f_{recv}$ \textbf{matches} $f_{local}$}
			\STATE $f_{local}.id = min(f_{recv}.id, f_{local}.id)$
			\STATE mark $f_{local}$ as not sent
		\ENDIF
	\ENDFOR	
\ENDFOR
\end{algorithmic}
\end{algorithm}
%------------------------------------------------

The naive solution may work for data sets with feature-on-boundary that span only a few number of blocks. However, if there exists long curly features partitioned evenly over the data block grid, each block needs to consecutively communicate with its neighbors to incrementally gather and merge a global feature. This requires O(${N_p}$) communications to connect a single feature, where ${N_p}$ is the number of processors in the grid. Consequently, the total communication cost is O(${N_{fb} \times N_p}$) times communication. In addition, since one processor cannot predict how many leaves it will receive from its adjacent processors, it is hard to schedule the communication process.

%------------------------------------------------
% \begin{figure*}[ht]
%   \centering
%     \subfigure[Local connectivity tree]{
%       \includegraphics[width=0.23\linewidth]{create.png}
%       \label{fig:create}
%     }
%     \subfigure[Centralized merge with host]{
%       \includegraphics[width=0.23\linewidth]{host.png}
%       \label{fig:host}
%     }
%     \subfigure[Centralized merge without host]{
%       \includegraphics[width=0.23\linewidth]{centralized.png}
%       \label{fig:centralized}
%     }
%     \subfigure[Decentralized merge]{
%       \includegraphics[width=0.23\linewidth]{decentralized.png}
%       \label{fig:decentralized}
%     }
%   \caption[Optional caption for list of figures]{Caption of subfigures \subref{fig:create}, \subref{fig:centralized} and \subref{fig:decentralized}}
% \end{figure*}
%------------------------------------------------

\subsubsection{The Centralized Approach}

One possible solution to improve the aforementioned \textcolor{red}{serialized} approach is to introduce master-slave hierarchy to reduce the number of communication required for merging all leaves. The master-slave hierarchy can be constructed using a separate host processor. When the feature extraction process is done, all local connectivity trees are gathered to the host processor. Then the host nodes starts to merge the leaves from each connectivity tree to construct a single global connectivity graph.

The merit of this centralized approach lies in that it requires inter-processor communication only once, ($N_{com} = 1$). Moreover, the global graph of feature information can be preserved in the host that it can response to feature queries directly without collecting information from the slaves again. However, this approach has an obvious drawback. Since all local connectivity trees are sent to the host, the number of processors involved in each communication is $N_{proc/com} = N_p$, and there exists potential bottlenecks, both in communication and computation, on the host.

\subsubsection{The Decentralized Approach}

A better solution is to decentralize the gathering and merging process from a single host processor to all processors available. After the feature extraction process is done and so does the creation of local connectivity tree, an \emph{all-gather} process starts to exchange all local connectivity trees within each data block to all the others. Each block will first collect a full copy of all local connectivity trees followed by the same process as mentioned in Algorithm~\ref{alg:merge} to merge the leaves into a single concise connectivity graph.

Though the "redundant" host processor is no longer required when applying for computation resources, this approach does not actually resolve the bottleneck problem since now every processor is acting like the host for that they still need to gather all local trees and to merge them \textcolor{red}{simultaneously}. For real world data set however, it is rarely the case that all features will span over every data block. In other word, it is unnecessary for one processor to gather leaves of features that are not local. To reduce the redundant communications with every other data block in the grid, the decentralized approach could be further improved to communicate with only those data blocks that are directly adjacent. That is, each data block only communicates with its direct neighbors and shares information with them regardless what is happening outside.

For a regularly partitioned volumetric data set, there are at most six direct neighbors for each data block. Instead of gathering connectivity information from all non-current data blocks in the grid, we let each data block gather only new leaves from its neighboring blocks within each communication. This could be considered as a higher level of region growing process, starts from one seeding block and grows to adjacent blocks by exchanging and merging connectivity information in a bread-first fashion, until all cross-boundary features are connected. Figure~\ref{fig:grid2d} gives an example of the process of processor-level region growing in a 2D processor grid.

%------------------------------------------------
\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{grid2d.png}
	\caption{The processor-level region growing process in a 2D grid, in which it takes a maximum of 2n-1 times (3n-1 for 3D grid) for the outermost processor to grow to the furthest processor}
	\label{fig:grid2d}
\end{figure}
%------------------------------------------------

The reason we choose the six-direct-neighbor paradigm is because it gives the minimum overall computational complexity. It takes a maximum of ${3n-1}$ times communication, where \emph{n} denotes the longest side length, for any data block to receive connectivity information from the furthest block on the opposite diagonal. The temporal complexity for garnering all necessary leaves is hence as low as ${O(\sqrt[3]{N_{proc}})}$. And the number of processors involved in each communication is a constant of a maximum of six ($N_{proc/com} \leq 6$).

Another optional paradigm is to let each processor communicate with its 26 neighbors, including the adjacent diagonal blocks. Communicate with the adjacent diagonal block takes as much as half the time for any block to reach its furthest diagonal. However, $N_{proc/com}$  is also increased to 26. For data sets that features span over all blocks are subordinate, six-direct-neighbor paradigm outweighs the 26-neighbors paradigm in computational complexity.

To schedule the communication with neighboring blocks, we use two schedule flags, \emph{toSend} and \emph{toRecv}, to indicate whether a block need to send or receive leaves to/from its neighboring blocks. Whenever a block receives leaves that can be merged into its existing connectivity graph, we assume the neighboring blocks might receive further connectivity information from their neighbors. In this case, the \emph{toRecv} flag is set to true until the connectivity graph is \textcolor{red}{saturated}; On the other hand, the \emph{toSend} flag is set in accordance to whether one of the neighboring blocks need to receive leaves, which can be obtained by gathering the neighboring \emph{toRecv} flags. Each data block will keep exchanging connectivity information until both \emph{toSend} and \emph{toRecv} flags are set to false.

The detailed scheduling algorithm is depicted in Algorithm~\ref{alg:schedule}.
%------------------------------------------------
\begin{algorithm}
\caption{Decentralized Local Merge}
\label{alg:schedule}
	\begin{algorithmic}
	\STATE $toSend, toRecv \leftarrow true$
	\STATE $\delta \leftarrow localLeaves$
	\WHILE {$toSend$ is true \textbf{or} $toRecv$ is true}
		\STATE $target \leftarrow myRank$ \textbf{if} $toRecv$ = $true$
		\STATE $procsToSync \leftarrow gatherNeighbor(target)$
		
		\FOR {each $proc$ in $procsToSync$}
			\IF {$toSend$ = $true$}
				\STATE send $\delta$ to $proc$
			\ENDIF
			\IF {$toRecv$ = $true$}
				\STATE receive $\delta\prime$ from $proc$
			\ENDIF
		\ENDFOR
		
		\STATE $toSend \leftarrow false$ \textbf{if} $procsToSync$ is empty \textbf{else} $true$
		\STATE $toRecv \leftarrow false$ \textbf{if} $\delta$ = $Merge(\delta, \delta\prime)$ \textbf{else} $true$
		\STATE $\delta \leftarrow Merge(\delta, \delta\prime)$ \textbf{if} $toRecv$ is true
  \ENDWHILE
  \end{algorithmic}
\end{algorithm}
%------------------------------------------------

\subsubsection{The hybrid approach}

Consider the fact that as data evolves over time, the volumetric features may drift but should not change drastically in neither size, shape nor location if the sampling interval when generating the data is sufficiently small. Base on this assumption, we can further optimizing the aforementioned decentralized-local-merge approach. Utilizing the prediction-correction approach for single processor feature tracking we can further reduces the communication cost required to complete the whole connectivity graph.

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{hybrid.png}
	\caption{Utilizing the prediction-correction for fast connectivity information synchronization. In each time step the }
	\label{fig:hybrid}
\end{figure}

As depicted in as depicted in Figure~\ref{fig:hybrid}, for every time step $t_i$, when the global connectivity graph is obtained, the local communicators will be updated for the next time step, $t_{i+1}$, with the union of processors that share the same set of feature-on-boundary with the current processor. The leaves from these data blocks are necessary to complete the global connectivity graph no matter which gathering approach is used. Hence, for these must-involve data blocks, we apply the all-gather-decentralize approach, allowing the minimum one-time synchronization to finish gathering all leaves that are necessary for updating the connectivity graph based on the graph created at the previous time step $t$. Then, processor-level region growing, a.k.a the processor-level-decentralize approach is applied to extend the boundary of data blocks, obtaining newly connected blocks caused by the evolution of the volume data. Again, only those leaves that are changed and not yet sent will be exchanged. This ensure that we minimize the amount of data being sent over network.

The detail algorithm of the hybrid approach is given in Algorithm~\ref{alg:hybrid}.
%------------------------------------------------
\begin{algorithm}
\caption{Prediction-enabled Local Merge}
\label{alg:hybrid}
	\begin{algorithmic}
		\STATE Let $P_{f_i} \equiv$ all processors that contains feature $f_i$
		\IF{$t = t_0$}
			\STATE $localCom \leftarrow union(P_{f_i})$ for all $f_i$ in current data block
		\ELSE
			\FOR {each $proc$ on boundary of $localCom$ \textbf{do}}
				\STATE $P_{f_i}^{'} \leftarrow decentralizedLocalMerge(proc)$
			\ENDFOR
			\STATE $localCom \leftarrow union(P_{f_i}^{'})$
		\ENDIF
	\end{algorithmic}
\end{algorithm}
%------------------------------------------------